{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dcf9d4f-5f92-4694-bd28-b3ee07c41313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import textwrap\n",
    "from typing import TypedDict, List, Dict, Optional, Any, Type\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e30259-0d4e-45bb-8bce-ee010202e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(name: str = \"interview_bot\") -> logging.Logger:\n",
    "    logger = logging.getLogger(name)\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler(sys.stdout)\n",
    "        formatter = logging.Formatter(\n",
    "            \"[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        )\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "logger = setup_logger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ac293bb-e5a3-42e1-ae76-676f65284a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760957632.479498    7946 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1760957632.483502    7946 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15cc8f38-1b1f-4541-b7d6-874e5866711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def chunk_cv_text(cv_text: str, user_id: str = \"default_user\") -> list:\n",
    "    \"\"\"Splits CV text into chunks for embedding and retrieval.\"\"\"\n",
    "    chunk_size = 800\n",
    "    chunk_overlap = 200\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    cv_text = cv_text.strip().replace(\"\\n\", \" \")\n",
    "    documents = []\n",
    "    \n",
    "    for i, chunk in enumerate(splitter.split_text(cv_text)):\n",
    "        chunk_text = chunk.strip()\n",
    "        if len(chunk_text) < 20:\n",
    "            continue\n",
    "        documents.append(Document(\n",
    "            page_content=chunk_text,\n",
    "            metadata={\"user_id\": user_id, \"chunk_index\": i}\n",
    "        ))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1b33f75-6ff8-4b5e-b3cd-791ed448d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vectorstore(documents: List[Document], user_id: str = \"default_user\"):\n",
    "    \"\"\"Create FAISS vectorstore from documents.\"\"\"\n",
    "    index_dir = os.path.join(os.getcwd(), f\"faiss_index_{user_id}\")\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    vectorstore.save_local(index_dir)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f4ac07-be6f-407e-94f8-43f3301590a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== RETRIEVAL DECISION =====\n",
    "\n",
    "def decide_retrieval(question: str, user_id: str = \"default_user\"):\n",
    "    \"\"\"Decides whether to retrieve context based on the question.\"\"\"\n",
    "    try:\n",
    "        index_dir = os.path.join(os.getcwd(), f\"faiss_index_{user_id}\")\n",
    "        if not os.path.exists(index_dir):\n",
    "            logger.warning(\"No FAISS index found, skipping retrieval.\")\n",
    "            return False, 1.0\n",
    "        \n",
    "        vectorstore = FAISS.load_local(\n",
    "            index_dir,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        \n",
    "        top_chunks = vectorstore.similarity_search_with_score(question, k=3)\n",
    "        if not top_chunks:\n",
    "            return False, 1.0\n",
    "\n",
    "        min_distance = min(score for _, score in top_chunks)\n",
    "        # Larger distance => less similar => need retrieval\n",
    "        needs_retrieval = min_distance > 0.55\n",
    "        \n",
    "        return needs_retrieval, min_distance\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Retrieval decision error: {e}\")\n",
    "        return False, 1.0\n",
    "\n",
    "# ===== STATE ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7acb21-edef-4bfb-9dec-6d091dea0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InterviewState(TypedDict):\n",
    "    topic: str\n",
    "    content: List[str]\n",
    "    cv_content: str\n",
    "    questions: List[str]\n",
    "    answers: List[str]\n",
    "    feedback: List[Dict]\n",
    "    current_question: Optional[str]\n",
    "    current_answer: Optional[str]\n",
    "    step: int\n",
    "    max_questions: int\n",
    "    final_evaluation: Optional[Dict]\n",
    "    messages: List[Dict]\n",
    "    question_type: str\n",
    "    needs_retrieval: bool\n",
    "    retrieved_context: Optional[str]\n",
    "    similarity_score: Optional[float]\n",
    "    user_id: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce6d4e3a-2909-4ba6-b9c1-ffea193097d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1760957632.554445    7946 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# ===== GEMINI CLIENT =====\n",
    "\n",
    "class QuestionFeedback(BaseModel):\n",
    "    rating: int = Field(0, ge=0, le=10)\n",
    "    feedback: str = \"No feedback\"\n",
    "\n",
    "class AnswerFeedback(BaseModel):\n",
    "    rating: int = Field(0, ge=0, le=10)\n",
    "    feedback: str = \"No feedback\"\n",
    "\n",
    "class GeminiClient:\n",
    "    \"\"\"Wrapper around Gemini LLM API.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "    def generate_content(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = self.model.invoke(prompt)\n",
    "            if hasattr(response, \"content\"):\n",
    "                return response.content.strip()\n",
    "            elif hasattr(response, \"text\"):\n",
    "                return response.text.strip()\n",
    "            else:\n",
    "                return str(response).strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini generation failed: {e}\")\n",
    "            return \"Tell me about your experience with this technology.\"\n",
    "    def safe_parse_json(\n",
    "        self, response_text: str, model: Type[BaseModel] = QuestionFeedback\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Safely parses a JSON string response from Gemini LLM and validates it using a Pydantic model.\n",
    "        Returns a default model dictionary if parsing or validation fails.\n",
    "\n",
    "        Args:\n",
    "            response_text (str): The raw JSON string returned by Gemini LLM.\n",
    "            model (Type[BaseModel], optional): Pydantic model class to validate JSON. Default is QuestionFeedback.\n",
    "\n",
    "        Returns:\n",
    "            dict: Validated dictionary according to the Pydantic model.\n",
    "        \"\"\"\n",
    "        if not response_text or not response_text.strip():\n",
    "            logger.warning(\"Empty response received; returning default model\")\n",
    "            return model().dict()\n",
    "\n",
    "        # Extract JSON substring from the response\n",
    "        match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                data = json.loads(match.group(0))\n",
    "                validated = model(**data)\n",
    "                return validated.dict()\n",
    "            except (json.JSONDecodeError, ValidationError) as e:\n",
    "                logger.error(f\"Failed to parse/validate JSON: {e}\")\n",
    "                return model().dict()\n",
    "        logger.warning(\"No JSON found in response; returning default model\")\n",
    "        return model().dict()\n",
    "\n",
    "\n",
    "gemini_client = GeminiClient()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e3cbcbb-fcb2-455e-9b39-fa322284bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_setup_prompt(topic: str, question_type: str) -> str:\n",
    "#     return f\"\"\"\n",
    "#     You are conducting a technical interview for a {topic} position.\n",
    "#     Generate an initial interview question that assesses basic knowledge and experience.\n",
    "#     \"\"\"\n",
    "\n",
    "# def get_rag_setup_prompt(topic: str, question_type: str, context: str) -> str:\n",
    "#     return f\"\"\"\n",
    "#     You are conducting a technical interview for a {topic} position.\n",
    "\n",
    "#     Candidate Background:\n",
    "#     {context}\n",
    "\n",
    "#     Generate the first interview question that considers the candidate's experience.\n",
    "#     \"\"\"\n",
    "\n",
    "# def get_question_generation_prompt(content: str, topic: str, step: int) -> str:\n",
    "#     return f\"\"\"\n",
    "#     Generate the next interview question for a {topic} position.\n",
    "\n",
    "#     Conversation so far:\n",
    "#     {content}\n",
    "\n",
    "#     Current step: {step + 1}\n",
    "\n",
    "#     Next question:\n",
    "#     \"\"\"\n",
    "\n",
    "# def get_rag_question_generation_prompt(content: str, topic: str, step: int, context: str) -> str:\n",
    "#     return f\"\"\"\n",
    "#     Generate the next interview question for a {topic} position.\n",
    "\n",
    "#     Candidate Background:\n",
    "#     {context}\n",
    "\n",
    "#     Conversation so far:\n",
    "#     {content}\n",
    "\n",
    "#     Current step: {step + 1}\n",
    "\n",
    "#     Next question:\n",
    "#     \"\"\"\n",
    "\n",
    "# # ===== UTIL =====\n",
    "\n",
    "# def safe_prompt(fstring: str) -> str:\n",
    "#     return textwrap.dedent(fstring).strip()\n",
    "\n",
    "# def _safe_generate(prompt: str, fallback: str) -> str:\n",
    "#     try:\n",
    "#         return gemini_client.generate_content(prompt) or fallback\n",
    "#     except Exception as e:\n",
    "#         logger.error(\"Generation failed: %s\", e)\n",
    "#         return fallback\n",
    "\n",
    "\"\"\"\n",
    "Interview System Prompts\n",
    "Centralized prompt templates for the AI Interviewer system\n",
    "\"\"\"\n",
    "\n",
    "import textwrap\n",
    "from typing import Literal\n",
    "\n",
    "# -------------------------------\n",
    "# Utility functions\n",
    "# -------------------------------\n",
    "\n",
    "def safe_text(text: str, max_len: int = 2000) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize and truncate user-provided or large text to avoid context overflow\n",
    "    and unwanted characters.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    sanitized = str(text).replace(\"\\r\", \"\").replace(\"\\t\", \"    \")\n",
    "    return sanitized[:max_len]\n",
    "\n",
    "def build_prompt(role_desc: str, content: str, body: str) -> str:\n",
    "    \"\"\"\n",
    "    Standard prompt builder to avoid duplication.\n",
    "    Applies safe_text to content and strips extra whitespace.\n",
    "    \"\"\"\n",
    "    return textwrap.dedent(f\"\"\"\n",
    "        You are {role_desc}.\n",
    "        Using the following reference content:\n",
    "        {safe_text(content)}\n",
    "\n",
    "        {body}\n",
    "    \"\"\").strip()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Setup and Initial Question Prompts\n",
    "# -------------------------------\n",
    "\n",
    "def get_setup_prompt(content_text: str, topic: str, question_type: str) -> str:\n",
    "    \"\"\"Prompt for generating the first question\"\"\"\n",
    "    question_style = \"Ask a broad, general question.\" if question_type.startswith('broad') else \"Ask a specific, detailed question.\"\n",
    "    body = f\"Generate question #1 for the topic: {topic}.\\n{question_style}\\nReturn ONLY the question text.\"\n",
    "    return build_prompt(\"an expert interviewer\", content_text, body)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Question Generation Prompts\n",
    "# -------------------------------\n",
    "\n",
    "def get_question_generation_prompt(content_text: str, prompt_instruction: str, topic: str, step: int) -> str:\n",
    "    \"\"\"Prompt for generating follow-up questions\"\"\"\n",
    "    body = f\"{prompt_instruction}\\nTopic: {topic}\\nQuestion number: {step + 1}\\nReturn ONLY the question text.\"\n",
    "    return build_prompt(\"an expert interviewer\", content_text, body)\n",
    "\n",
    "def get_question_instruction(is_followup: bool, is_broad: bool, previous_answer: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Generate the instruction part for question generation.\n",
    "    Avoids awkward references when previous_answer is empty.\n",
    "    \"\"\"\n",
    "    style = \"broad, general\" if is_broad else \"specific, detailed\"\n",
    "    scope = \"follow-up\" if is_followup else \"new aspect\"\n",
    "\n",
    "    if is_followup and previous_answer.strip():\n",
    "        return f\"Generate a {style} {scope} question that directly probes details from the previous answer: {previous_answer}\"\n",
    "    elif is_followup:\n",
    "        return f\"Generate a {style} {scope} question that builds on the previous discussion.\"\n",
    "    else:\n",
    "        return f\"Generate a {style} {scope} question that explores a new aspect of the topic, independent of the previous answer.\"\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Generic Evaluation Prompt\n",
    "# -------------------------------\n",
    "\n",
    "def get_evaluation_prompt(\n",
    "    kind: Literal[\"question\", \"answer\"],\n",
    "    full_messages: str,\n",
    "    full_content: str,\n",
    "    transcript: str,\n",
    "    last_question: str = \"\",\n",
    "    last_answer: str = \"\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generic evaluation prompt for either 'question' or 'answer'.\n",
    "    Sanitizes all inputs and prevents exceeding context.\n",
    "    \"\"\"\n",
    "    kind_desc = \"question\" if kind == \"question\" else \"candidate answer\"\n",
    "\n",
    "    body = textwrap.dedent(f\"\"\"\n",
    "        Evaluate the following {kind_desc} for its clarity, relevance, depth, and alignment with the topic,\n",
    "        considering the ENTIRE interview history, accumulated context, all previous messages, questions, answers, and feedback.\n",
    "\n",
    "        Full Interview History (Messages): {safe_text(full_messages)}\n",
    "        Accumulated Context (Search Snippets): {safe_text(full_content)}\n",
    "        Previous Q&A Transcript: {safe_text(transcript)}\n",
    "        Current Question: {safe_text(last_question)}\n",
    "        Current Candidate Answer: {safe_text(last_answer)}\n",
    "\n",
    "        Provide a rating (1-10) for {kind_desc} quality and detailed feedback. Return JSON only, no extra text\n",
    "    \"\"\")\n",
    "\n",
    "    if kind == \"answer\":\n",
    "        body += textwrap.dedent(\"\"\"\n",
    "            Return in JSON format:\n",
    "            {\n",
    "                \"rating\": 0,\n",
    "                \"feedback\": \"...\"\n",
    "            }\n",
    "        \"\"\")\n",
    "\n",
    "    return build_prompt(\"an expert interviewer\", \"\", body)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Final Evaluation Prompt\n",
    "# -------------------------------\n",
    "\n",
    "def get_final_evaluation_prompt(transcript: str) -> str:\n",
    "    \"\"\"Prompt for final evaluation of all questions\"\"\"\n",
    "    body = textwrap.dedent(f\"\"\"\n",
    "        Based on this transcript, produce a JSON summary evaluation of the questions:\n",
    "        {safe_text(transcript)}\n",
    "        return only JSON, no extra text\n",
    "        JSON format ONLY, with explicit types:\n",
    "        {{\n",
    "            \"overall_quality\": 0,             # integer 1-10\n",
    "            \"strengths\": [\"...\"],             # list of strings\n",
    "            \"areas_for_improvement\": [\"...\"], # list of strings\n",
    "            \"recommendation\": \"...\",          # string: keep/revise/remove\n",
    "            \"final_feedback\": \"...\"           # string\n",
    "        }}\n",
    "    \"\"\").strip()\n",
    "    return build_prompt(\"an expert interviewer\", \"\", body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23813f3c-c8a3-476e-9d3e-ce7ba0f20146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agentic RAG interview nodes (clean, structured, and printable report)\n",
    "Drop this into your nodes module (replace the corresponding functions).\n",
    "Assumes external symbols exist:\n",
    " - InterviewState (mapping-like), gemini_client, FAISS, embeddings\n",
    " - decide_retrieval, _safe_generate, safe_prompt\n",
    " - get_setup_prompt, get_rag_setup_prompt, get_question_generation_prompt,\n",
    "   get_rag_question_generation_prompt, get_question_instruction,\n",
    "   get_evaluation_prompt, get_final_evaluation_prompt\n",
    " - setup_logger\n",
    "If any of those are missing in your environment, import or adapt accordingly.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import textwrap\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "\n",
    "# -------------------------\n",
    "# Logger (reuse your setup)\n",
    "# -------------------------\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "# -------------------------\n",
    "# Helper utilities\n",
    "# -------------------------\n",
    "def safe_parse_json(response: Any) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Robustly parse possible model responses into a Python dict.\n",
    "    Handles:\n",
    "      - dict-like responses\n",
    "      - objects with `.text` attribute\n",
    "      - strings that contain JSON (extract first {...} substring)\n",
    "    Falls back to a minimal safe structure if parsing fails.\n",
    "    \"\"\"\n",
    "    fallback = {\"rating\": 6, \"feedback\": \"Good effort. Could elaborate more.\"}\n",
    "\n",
    "    if not response:\n",
    "        return fallback\n",
    "\n",
    "    # If it's already a dict-like object, return as-is\n",
    "    if isinstance(response, dict):\n",
    "        return response\n",
    "\n",
    "    # If it's an object with .text or .content, try to pull its string\n",
    "    text = None\n",
    "    if hasattr(response, \"text\"):\n",
    "        try:\n",
    "            text = response.text\n",
    "        except Exception:\n",
    "            text = None\n",
    "    if text is None and hasattr(response, \"content\"):\n",
    "        try:\n",
    "            text = response.content\n",
    "        except Exception:\n",
    "            text = None\n",
    "    if text is None and isinstance(response, str):\n",
    "        text = response\n",
    "\n",
    "    if not text:\n",
    "        return fallback\n",
    "\n",
    "    # Try full JSON parse\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Try to extract substring that looks like JSON object\n",
    "    try:\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\") + 1\n",
    "        if start != -1 and end != -1 and end > start:\n",
    "            json_str = text[start:end]\n",
    "            return json.loads(json_str)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # As a last attempt, return fallback but include the raw text for debugging\n",
    "    return {\"rating\": 6, \"feedback\": \"Good effort. Could elaborate more.\", \"raw_text\": text[:1000]}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FinalEvaluation:\n",
    "    \"\"\"\n",
    "    Structured final evaluation object used to store overall summary.\n",
    "    \"\"\"\n",
    "    overall_quality: int\n",
    "    strengths: List[str]\n",
    "    areas_for_improvement: List[str]\n",
    "    recommendation: str\n",
    "    final_feedback: str\n",
    "\n",
    "    def model_dump(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"overall_quality\": int(self.overall_quality),\n",
    "            \"strengths\": list(self.strengths or []),\n",
    "            \"areas_for_improvement\": list(self.areas_for_improvement or []),\n",
    "            \"recommendation\": str(self.recommendation),\n",
    "            \"final_feedback\": str(self.final_feedback),\n",
    "        }\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Nodes\n",
    "# -------------------------\n",
    "\n",
    "# ---------- SETUP NODE ----------\n",
    "def setup_node(state: Mapping[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Initialize interview state, optionally run RAG retrieval for topic.\n",
    "    \"\"\"\n",
    "    topic = state.get(\"topic\", \"\").strip()\n",
    "    question_type = state.get(\"question_type\", \"broad_followup\").strip()\n",
    "    user_id = state.get(\"user_id\", \"default_user\")\n",
    "\n",
    "    needs_retrieval = True\n",
    "    retrieved_context = \"\"\n",
    "    similarity_score = 0.0\n",
    "\n",
    "    if needs_retrieval:\n",
    "        try:\n",
    "            index_dir = os.path.join(os.getcwd(), f\"faiss_index_{user_id}\")\n",
    "            vectorstore = FAISS.load_local(index_dir, embeddings, allow_dangerous_deserialization=True)\n",
    "            docs = vectorstore.similarity_search(topic, k=3)\n",
    "            retrieved_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "            logger.info(\"Retrieved context for setup based on topic: %s\", topic)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Setup retrieval failed: %s\", e)\n",
    "\n",
    "    # prompt builder expects (content_text, topic, question_type)\n",
    "    if retrieved_context:\n",
    "        prompt = safe_prompt(get_setup_prompt(retrieved_context, topic, question_type))\n",
    "    else:\n",
    "        prompt = safe_prompt(get_setup_prompt(\"\", topic, question_type))\n",
    "\n",
    "    first_question = _safe_generate(prompt, \"Tell me about your experience with this technology.\")\n",
    "\n",
    "    new_state = {\n",
    "        **dict(state),\n",
    "        \"topic\": topic,\n",
    "        \"question_type\": question_type,\n",
    "        \"content\": [retrieved_context or \"No content\"],\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": f\"Interview topic: {topic}\"}],\n",
    "        \"step\": 0,\n",
    "        \"questions\": [],\n",
    "        \"answers\": [],\n",
    "        \"feedback\": [],\n",
    "        \"current_question\": first_question,\n",
    "        \"max_questions\": state.get(\"max_questions\", 3),\n",
    "        \"needs_retrieval\": needs_retrieval,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"similarity_score\": similarity_score,\n",
    "    }\n",
    "    return new_state\n",
    "\n",
    "\n",
    "# ---------- GET ANSWER NODE ----------\n",
    "def get_answer_node(state: Mapping[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Collect user's answer (CLI) and update messages/transcript.\n",
    "    Replace blocking input() if using a web frontend.\n",
    "    \"\"\"\n",
    "    current_q = state.get(\"current_question\")\n",
    "    if not current_q:\n",
    "        raise ValueError(\"No current_question found in state.\")\n",
    "\n",
    "    answer = input(f\"\\n❓ Question {state.get('step', 0) + 1}: {current_q}\\n💭 Your answer: \").strip()\n",
    "\n",
    "    new_messages = list(state.get(\"messages\", [])) + [\n",
    "        {\"role\": \"interviewer\", \"content\": current_q},\n",
    "        {\"role\": \"candidate\", \"content\": answer},\n",
    "    ]\n",
    "\n",
    "    content_list = list(state.get(\"content\", []))\n",
    "    content_list.append(f\"Q: {current_q}\\nA: {answer}\")\n",
    "\n",
    "    return {\n",
    "        **dict(state),\n",
    "        \"current_answer\": answer,\n",
    "        \"messages\": new_messages,\n",
    "        \"questions\": list(state.get(\"questions\", [])) + [current_q],\n",
    "        \"answers\": list(state.get(\"answers\", [])) + [answer],\n",
    "        \"content\": content_list,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- RETRIEVAL DECISION NODE ----------\n",
    "def retrieval_decision_node(state: Mapping[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Decide whether to retrieve additional context based on the current answer.\n",
    "    Uses `decide_retrieval(answer, user_id)` - implement that heuristic as needed.\n",
    "    \"\"\"\n",
    "    current_answer = state.get(\"current_answer\", \"\")\n",
    "    user_id = state.get(\"user_id\", \"default_user\")\n",
    "\n",
    "    if not current_answer:\n",
    "        return {**dict(state), \"needs_retrieval\": False, \"retrieved_context\": None, \"similarity_score\": 0}\n",
    "\n",
    "    needs_retrieval, similarity_score = decide_retrieval(current_answer, user_id)\n",
    "    retrieved_context = None\n",
    "\n",
    "    if needs_retrieval:\n",
    "        try:\n",
    "            index_dir = os.path.join(os.getcwd(), f\"faiss_index_{user_id}\")\n",
    "            vectorstore = FAISS.load_local(index_dir, embeddings, allow_dangerous_deserialization=True)\n",
    "            docs = vectorstore.similarity_search(current_answer, k=3)\n",
    "            retrieved_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "            logger.info(\"Retrieved context based on answer similarity: %.2f\", similarity_score)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Answer retrieval failed: %s\", e)\n",
    "\n",
    "    return {\n",
    "        **dict(state),\n",
    "        \"needs_retrieval\": needs_retrieval,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"similarity_score\": similarity_score,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- EVALUATE QUESTION NODE ----------\n",
    "def evaluate_question_node(state: Mapping[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Use the LLM to evaluate both the question quality and the candidate's answer.\n",
    "    Stores structured feedback in state['feedback'].\n",
    "    \"\"\"\n",
    "    questions = list(state.get(\"questions\", []))\n",
    "    answers = list(state.get(\"answers\", []))\n",
    "    if not questions or not answers:\n",
    "        logger.warning(\"No questions/answers to evaluate.\")\n",
    "        return dict(state)\n",
    "\n",
    "    current_q = questions[-1]\n",
    "    current_a = answers[-1]\n",
    "    full_content = \"\\n\".join(state.get(\"content\", []))\n",
    "    transcript = \"\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in zip(questions, answers)])\n",
    "    messages_text = \"\\n\".join([m.get(\"content\", \"\") for m in state.get(\"messages\", [])])\n",
    "\n",
    "    try:\n",
    "        # Ask for a question-quality evaluation\n",
    "        q_prompt = get_evaluation_prompt(\n",
    "            kind=\"question\",\n",
    "            full_messages=messages_text,\n",
    "            full_content=full_content,\n",
    "            transcript=transcript,\n",
    "            last_question=current_q,\n",
    "            last_answer=current_a,\n",
    "        )\n",
    "        q_raw = gemini_client.generate_content(q_prompt)\n",
    "        q_parsed = safe_parse_json(q_raw)\n",
    "\n",
    "        # Ask for an answer evaluation\n",
    "        a_prompt = get_evaluation_prompt(\n",
    "            kind=\"answer\",\n",
    "            full_messages=messages_text,\n",
    "            full_content=full_content,\n",
    "            transcript=transcript,\n",
    "            last_question=current_q,\n",
    "            last_answer=current_a,\n",
    "        )\n",
    "        a_raw = gemini_client.generate_content(a_prompt)\n",
    "        a_parsed = safe_parse_json(a_raw)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Gemini feedback parsing failed: %s\", e)\n",
    "        q_parsed = {\"rating\": 6, \"feedback\": \"Good effort. Could elaborate more.\"}\n",
    "        a_parsed = {\"rating\": 6, \"feedback\": \"Good effort. Could elaborate more.\"}\n",
    "\n",
    "    # Normalise field names to your earlier schema if necessary\n",
    "    # e.g., ensure 'rating' or 'score' consistently present\n",
    "    def _norm(parsed: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        out = dict(parsed)\n",
    "        if \"score\" in parsed and \"rating\" not in parsed:\n",
    "            out[\"rating\"] = parsed[\"score\"]\n",
    "        if \"rating\" in parsed and isinstance(parsed[\"rating\"], (float, str)):\n",
    "            try:\n",
    "                out[\"rating\"] = int(parsed[\"rating\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "        return out\n",
    "\n",
    "    q_final = _norm(q_parsed)\n",
    "    a_final = _norm(a_parsed)\n",
    "\n",
    "    # Append to feedback list\n",
    "    feedback_list = list(state.get(\"feedback\", []))\n",
    "    feedback_list.append({\"question_feedback\": q_final, \"answer_feedback\": a_final})\n",
    "\n",
    "    logger.info(\"Question %d evaluated: %s\", state.get(\"step\", 0) + 1, a_final.get(\"feedback\", \"\"))\n",
    "\n",
    "    return {**dict(state), \"feedback\": feedback_list, \"step\": state.get(\"step\", 0) + 1}\n",
    "\n",
    "\n",
    "# ---------- GENERATE QUESTION NODE ----------\n",
    "def generate_question_node(state: Mapping[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate the next question. Uses RAG prompt if retrieved_context present.\n",
    "    Adapts behavior according to `state['question_type']`.\n",
    "    \"\"\"\n",
    "    step = state.get(\"step\", 0)\n",
    "    max_questions = state.get(\"max_questions\", 3)\n",
    "    if step >= max_questions:\n",
    "        logger.warning(\"Max questions reached, skipping question generation.\")\n",
    "        return dict(state)\n",
    "\n",
    "    topic = state.get(\"topic\", \"\")\n",
    "    content_list = list(state.get(\"content\", [\"No content\"]))\n",
    "    prompt_instruction = get_question_instruction(\n",
    "        is_followup=True if \"followup\" in state.get(\"question_type\", \"\") else False,\n",
    "        is_broad=True if \"broad\" in state.get(\"question_type\", \"\") else False,\n",
    "        previous_answer=state.get(\"current_answer\", \"\")\n",
    "    )\n",
    "\n",
    "    retrieved_context = state.get(\"retrieved_context\")\n",
    "    if retrieved_context and state.get(\"needs_retrieval\", False):\n",
    "        prompt = safe_prompt(get_rag_question_generation_prompt(\"\\n\".join(content_list), topic, step, retrieved_context))\n",
    "        logger.info(\"Using RAG context for question generation\")\n",
    "    else:\n",
    "        prompt = safe_prompt(get_question_generation_prompt(\"\\n\".join(content_list), prompt_instruction, topic, step))\n",
    "        logger.info(\"Using standard question generation\")\n",
    "\n",
    "    question = _safe_generate(prompt, f\"Tell me more about your experience with {topic}.\")\n",
    "    return {**dict(state), \"current_question\": question}\n",
    "\n",
    "\n",
    "# ---------- FINAL EVALUATION NODE ----------\n",
    "def final_evaluation_node(state: Mapping[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Summarize all feedback and produce a final structured evaluation.\n",
    "    \"\"\"\n",
    "    questions = list(state.get(\"questions\", []))\n",
    "    answers = list(state.get(\"answers\", []))\n",
    "    feedback = list(state.get(\"feedback\", []))\n",
    "\n",
    "    if not questions or not answers:\n",
    "        logger.warning(\"No data available for final evaluation.\")\n",
    "        return dict(state)\n",
    "\n",
    "    # Compose transcript for final prompt\n",
    "    transcript = \"\"\n",
    "    for i in range(len(questions)):\n",
    "        q = questions[i]\n",
    "        a = answers[i] if i < len(answers) else \"\"\n",
    "        fb = feedback[i] if i < len(feedback) else {}\n",
    "        transcript += f\"Q{i+1}: {q}\\nA{i+1}: {a}\\nFeedback: {fb.get('answer_feedback', {}).get('feedback', '')}\\n\\n\"\n",
    "\n",
    "    final_prompt = get_final_evaluation_prompt(transcript)\n",
    "    try:\n",
    "        raw_final = gemini_client.generate_content(final_prompt)\n",
    "        parsed_final = safe_parse_json(raw_final)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Final evaluation parsing failed: %s\", e)\n",
    "        parsed_final = {}\n",
    "\n",
    "    final_eval = FinalEvaluation(\n",
    "        overall_quality=int(parsed_final.get(\"overall_quality\", 7)),\n",
    "        strengths=parsed_final.get(\"strengths\", [\"Good technical depth\"]),\n",
    "        areas_for_improvement=parsed_final.get(\"areas_for_improvement\", [\"Could elaborate on examples\"]),\n",
    "        recommendation=parsed_final.get(\"recommendation\", \"Recommended with reservations.\"),\n",
    "        final_feedback=parsed_final.get(\"final_feedback\", \"Solid overall performance with scope for improvement.\"),\n",
    "    )\n",
    "\n",
    "    logger.info(\"Final evaluation completed successfully.\")\n",
    "    return {**dict(state), \"final_evaluation\": final_eval.model_dump()}\n",
    "\n",
    "\n",
    "# ---------- DISPLAY RESULTS NODE ----------\n",
    "def display_results_node(state: Mapping[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Pretty-print full interview report to console and save structured JSON to disk.\n",
    "    \"\"\"\n",
    "    topic = state.get(\"topic\", \"N/A\")\n",
    "    user_id = state.get(\"user_id\", \"user\")\n",
    "    questions = list(state.get(\"questions\", []))\n",
    "    answers = list(state.get(\"answers\", []))\n",
    "    feedback = list(state.get(\"feedback\", []))\n",
    "    final_eval = state.get(\"final_evaluation\", {})\n",
    "\n",
    "    # Pretty terminal output\n",
    "    sep = \"=\" * 70\n",
    "    print(\"\\n\" + sep)\n",
    "    print(f\"INTERVIEW REPORT — Topic: {topic}\")\n",
    "    print(sep + \"\\n\")\n",
    "\n",
    "    for i, q in enumerate(questions, start=1):\n",
    "        a = answers[i - 1] if i - 1 < len(answers) else \"\"\n",
    "        fb = feedback[i - 1].get(\"answer_feedback\", {}) if i - 1 < len(feedback) else {}\n",
    "        q_fb = feedback[i - 1].get(\"question_feedback\", {}) if i - 1 < len(feedback) else {}\n",
    "        print(f\"Q{i}: {q}\")\n",
    "        print(f\"A{i}: {a}\\n\")\n",
    "        print(\"Question Feedback:\")\n",
    "        print(f\"  {q_fb.get('feedback', q_fb.get('comment', 'No feedback'))}\")\n",
    "        print(f\"  (Rating: {q_fb.get('rating', q_fb.get('score', '-'))})\\n\")\n",
    "        print(\"Answer Feedback:\")\n",
    "        print(f\"  {fb.get('feedback', 'No feedback')}\")\n",
    "        # if suggestions exist, show them\n",
    "        suggestions = fb.get(\"suggestions\") or fb.get(\"recommendations\") or []\n",
    "        if suggestions:\n",
    "            print(\"  Suggestions:\")\n",
    "            for s in suggestions:\n",
    "                print(f\"   - {s}\")\n",
    "        print(f\"  (Rating: {fb.get('rating', fb.get('score', '-'))})\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "    # Final evaluation summary\n",
    "    print(\"\\nFINAL EVALUATION\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Overall Quality: {final_eval.get('overall_quality', 'N/A')}/10\")\n",
    "    print(\"Strengths:\")\n",
    "    for s in final_eval.get(\"strengths\", []):\n",
    "        print(f\" - {s}\")\n",
    "    print(\"Areas for improvement:\")\n",
    "    for a in final_eval.get(\"areas_for_improvement\", []):\n",
    "        print(f\" - {a}\")\n",
    "    print(f\"Recommendation: {final_eval.get('recommendation', 'N/A')}\")\n",
    "    print(f\"\\nFinal Feedback: {final_eval.get('final_feedback', '')}\")\n",
    "    print(sep + \"\\n\")\n",
    "\n",
    "    # Save JSON result\n",
    "    try:\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"results/{user_id}_{timestamp}.json\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dict(state), f, indent=2, ensure_ascii=False)\n",
    "        logger.info(\"Results saved to %s\", filename)\n",
    "        print(f\"Results saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to save results: %s\", e)\n",
    "        print(\"Failed to save results:\", e)\n",
    "\n",
    "    return dict(state)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# End of nodes module\n",
    "# -------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a343ffc-dc42-49f2-bcab-22ef73a1b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== GRAPH CONSTRUCTION =====\n",
    "\n",
    "SETUP_NODE = \"setup\"\n",
    "GET_ANSWER_NODE = \"get_answer\"\n",
    "RETRIEVAL_DECISION_NODE = \"retrieval_decision\"\n",
    "EVALUATE_QUESTION_NODE = \"evaluate_question\"\n",
    "GENERATE_QUESTION_NODE = \"generate_question\"\n",
    "FINAL_EVALUATION_NODE = \"final_evaluation\"\n",
    "DISPLAY_RESULTS_NODE = \"display_results\"\n",
    "\n",
    "def should_continue(state: InterviewState) -> str:\n",
    "    step = state.get(\"step\", 0)\n",
    "    max_questions = state.get(\"max_questions\", 3)\n",
    "    return GENERATE_QUESTION_NODE if step < max_questions else FINAL_EVALUATION_NODE\n",
    "\n",
    "def create_interview_graph() -> StateGraph:\n",
    "    logger.info(\"Initializing interview graph with RAG...\")\n",
    "\n",
    "    builder = StateGraph(InterviewState)\n",
    "    builder.add_node(SETUP_NODE, setup_node)\n",
    "    builder.add_node(GET_ANSWER_NODE, get_answer_node)\n",
    "    builder.add_node(RETRIEVAL_DECISION_NODE, retrieval_decision_node)\n",
    "    builder.add_node(EVALUATE_QUESTION_NODE, evaluate_question_node)\n",
    "    builder.add_node(GENERATE_QUESTION_NODE, generate_question_node)\n",
    "    builder.add_node(FINAL_EVALUATION_NODE, final_evaluation_node)\n",
    "    builder.add_node(DISPLAY_RESULTS_NODE, display_results_node)\n",
    "\n",
    "    builder.set_entry_point(SETUP_NODE)\n",
    "    builder.add_edge(SETUP_NODE, GET_ANSWER_NODE)\n",
    "    builder.add_edge(GET_ANSWER_NODE, RETRIEVAL_DECISION_NODE)\n",
    "    builder.add_edge(RETRIEVAL_DECISION_NODE, EVALUATE_QUESTION_NODE)\n",
    "    builder.add_conditional_edges(\n",
    "        EVALUATE_QUESTION_NODE,\n",
    "        should_continue,\n",
    "        {\n",
    "            GENERATE_QUESTION_NODE: GENERATE_QUESTION_NODE,\n",
    "            FINAL_EVALUATION_NODE: FINAL_EVALUATION_NODE,\n",
    "        },\n",
    "    )\n",
    "    builder.add_edge(GENERATE_QUESTION_NODE, GET_ANSWER_NODE)\n",
    "    builder.add_edge(FINAL_EVALUATION_NODE, DISPLAY_RESULTS_NODE)\n",
    "    builder.add_edge(DISPLAY_RESULTS_NODE, END)\n",
    "\n",
    "    logger.info(\"Interview graph with RAG successfully constructed.\")\n",
    "    return builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3e3f06f-d608-491e-bb69-48e35c3cc2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter CV path:  cv3.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CV processed: 7 chunks created and indexed.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter interview topic/job title:  python senior dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Choose question style:\n",
      "1. Broad, follow-up questions (general, builds on previous answers)\n",
      "2. Narrow, follow-up questions (specific, probes details from previous answers)\n",
      "3. Broad, non-follow-up questions (general, new topic aspects)\n",
      "4. Narrow, non-follow-up questions (specific, new topic aspects)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-4):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-20 13:54:10] [INFO] [__main__] Initializing interview graph with RAG...\n",
      "[2025-10-20 13:54:10] [INFO] [__main__] Interview graph with RAG successfully constructed.\n",
      "\n",
      "🚀 Starting interview...\n",
      "\n",
      "[2025-10-20 13:54:10] [INFO] [__main__] Retrieved context for setup based on topic: python senior dev\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'safe_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     35\u001b[39m initial_state = {\n\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m: topic,\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser123\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m }\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🚀 Starting interview...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m final_state = \u001b[43minterview_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Interview completed! Results saved in interview_results.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lg-ev/lib/python3.11/site-packages/langgraph/pregel/main.py:3085\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3082\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3083\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3085\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3090\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3091\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3092\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3093\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3094\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3100\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lg-ev/lib/python3.11/site-packages/langgraph/pregel/main.py:2674\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2672\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2673\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2679\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2684\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lg-ev/lib/python3.11/site-packages/langgraph/pregel/_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lg-ev/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lg-ev/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lg-ev/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36msetup_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# prompt builder expects (content_text, topic, question_type)\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retrieved_context:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     prompt = \u001b[43msafe_prompt\u001b[49m(get_setup_prompt(retrieved_context, topic, question_type))\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    137\u001b[39m     prompt = safe_prompt(get_setup_prompt(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, topic, question_type))\n",
      "\u001b[31mNameError\u001b[39m: name 'safe_prompt' is not defined",
      "During task with name 'setup' and id 'f7b4a187-904d-2c99-7748-4cca45d480e2'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== MAIN EXECUTION =====\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cv_path = input(\"Enter CV path: \").strip()\n",
    "    cv_text = extract_text_from_pdf(cv_path)\n",
    "    documents = chunk_cv_text(cv_text, user_id=\"user123\")\n",
    "    vectorstore = create_vectorstore(documents, user_id=\"user123\")\n",
    "    print(f\"✅ CV processed: {len(documents)} chunks created and indexed.\")\n",
    "\n",
    "    topic = input(\"Enter interview topic/job title: \").strip()\n",
    "    print(\"\\nChoose question style:\")\n",
    "    print(\"1. Broad, follow-up questions (general, builds on previous answers)\")\n",
    "    print(\n",
    "        \"2. Narrow, follow-up questions (specific, probes details from previous answers)\"\n",
    "    )\n",
    "    print(\"3. Broad, non-follow-up questions (general, new topic aspects)\")\n",
    "    print(\"4. Narrow, non-follow-up questions (specific, new topic aspects)\")\n",
    "\n",
    "    question_type_map = {\n",
    "        \"1\": \"broad_followup\",\n",
    "        \"2\": \"narrow_followup\",\n",
    "        \"3\": \"broad_nonfollowup\",\n",
    "        \"4\": \"narrow_nonfollowup\",\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        choice = input(\"Enter choice (1-4): \").strip()\n",
    "        if choice in question_type_map:\n",
    "            question_type = question_type_map[choice]\n",
    "            break\n",
    "        print(\"⚠️ Invalid choice! Please enter 1, 2, 3, or 4.\")\n",
    "\n",
    "    interview_graph = create_interview_graph()\n",
    "\n",
    "    initial_state = {\n",
    "        \"topic\": topic,\n",
    "        \"content\": [],\n",
    "        \"cv_content\": cv_text[:1000],\n",
    "        \"questions\": [],\n",
    "        \"answers\": [],\n",
    "        \"feedback\": [],\n",
    "        \"current_question\": None,\n",
    "        \"current_answer\": None,\n",
    "        \"step\": 0,\n",
    "        \"max_questions\": 3,\n",
    "        \"final_evaluation\": None,\n",
    "        \"messages\": [],\n",
    "        \"question_type\": question_type,\n",
    "        \"needs_retrieval\": False,\n",
    "        \"retrieved_context\": None,\n",
    "        \"similarity_score\": None,\n",
    "        \"user_id\": \"user123\"\n",
    "    }\n",
    "\n",
    "    print(\"\\n🚀 Starting interview...\\n\")\n",
    "    final_state = interview_graph.invoke(initial_state)\n",
    "    print(\"\\n✅ Interview completed! Results saved in interview_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc86f9c-8830-46e1-98a4-83d34b00a9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
