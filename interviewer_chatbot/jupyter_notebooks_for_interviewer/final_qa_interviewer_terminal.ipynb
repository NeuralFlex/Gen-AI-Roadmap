{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "911385cf-b4a2-407f-8283-935b23657f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import TypedDict, List, Dict, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "import google.generativeai as genai\n",
    "from tavily import TavilyClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dacef1a-ede0-4853-948d-849708ce432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1b04d7-41eb-4391-82ab-ae663f301155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not gemini_api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in environment variables!\")\n",
    "genai.configure(api_key=gemini_api_key)\n",
    "\n",
    "if not tavily_api_key:\n",
    "    raise ValueError(\"TAVILY_API_KEY not found in environment variables!\")\n",
    "tavily = TavilyClient(api_key=tavily_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc1627f-e132-423e-8f2b-b4cffba4b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Helper Functions ----\n",
    "def call_gemini(prompt: str, retries=3, delay=5) -> str:\n",
    "    \"\"\"Call Gemini API with retry logic.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            model = genai.GenerativeModel(\"gemini-2.5-flash\")  # Stable model\n",
    "            response = model.generate_content(prompt)\n",
    "            return response.text.strip() if response.text else \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Gemini API error (attempt {attempt+1}/{retries}): {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                import time\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee3e111e-2dde-4cab-b9fc-5efb854003c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_tavily(query: str) -> list[str]:\n",
    "    \"\"\"Return up to 5 Tavily search result snippets as a list of strings.\"\"\"\n",
    "    try:\n",
    "        response = tavily.search(query=query, top_k=5)\n",
    "        results = response.get(\"results\", [])\n",
    "        return [r.get(\"snippet\") or r.get(\"content\") or r.get(\"title\", \"\") for r in results][:5]\n",
    "    except Exception as e:\n",
    "        print(f\"Tavily search failed: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "046e06c6-1900-4770-9807-45ad61a57fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safe_parse_json(response_text: str, fallback: Dict = None) -> Dict:\n",
    "    \"\"\"Safely parse JSON with fallbacks for empty/invalid responses.\"\"\"\n",
    "    if not response_text or not response_text.strip():\n",
    "        print(\"Warning: Empty API response - using fallback.\")\n",
    "        return fallback or {\"error\": \"Empty response\"}\n",
    "    try:\n",
    "        return json.loads(response_text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON Parse Error: {e}\")\n",
    "        match = re.search(r\"\\{.*\\}\", response_text, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(0))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        return fallback or {\"error\": f\"Invalid response: {response_text[:100]}...\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "083b4b0d-f9ae-4992-b93f-750327f671ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- State Definition ----\n",
    "class InterviewState(TypedDict):\n",
    "    topic: str\n",
    "    content: List[str]\n",
    "    questions: List[str]\n",
    "    answers: List[str]\n",
    "    feedback: List[Dict]\n",
    "    current_question: Optional[str]\n",
    "    current_answer: Optional[str]\n",
    "    step: int\n",
    "    max_questions: int\n",
    "    final_evaluation: Optional[Dict]\n",
    "    messages: List[Dict]\n",
    "    question_type: str  # \"broad_followup\", \"narrow_followup\", \"broad_nonfollowup\", \"narrow_nonfollowup\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a509642b-a178-42c0-be08-2bbbcd7ed995",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Nodes ----\n",
    "def setup_node(state: InterviewState) -> InterviewState:\n",
    "    print(\" Welcome to the AI Interviewer (Question Evaluation Mode)!\")\n",
    "    topic = input(\"Enter the interview topic: \").strip()\n",
    "    \n",
    "    # Prompt for question type preference\n",
    "    print(\"\\nChoose question style:\")\n",
    "    print(\"1. Broad, follow-up questions (general, builds on previous answers)\")\n",
    "    print(\"2. Narrow, follow-up questions (specific, probes details from previous answers)\")\n",
    "    print(\"3. Broad, non-follow-up questions (general, new topic aspects)\")\n",
    "    print(\"4. Narrow, non-follow-up questions (specific, new topic aspects)\")\n",
    "    choice = input(\"Enter choice (1-4): \").strip()\n",
    "    question_type_map = {\n",
    "        \"1\": \"broad_followup\",\n",
    "        \"2\": \"narrow_followup\",\n",
    "        \"3\": \"broad_nonfollowup\",\n",
    "        \"4\": \"narrow_nonfollowup\"\n",
    "    }\n",
    "    question_type = question_type_map.get(choice, \"broad_followup\")  # Default to broad_followup\n",
    "\n",
    "    content_list = search_tavily(f\"key areas for interview on: {topic}\")\n",
    "    initial_messages = [{\"role\": \"user\", \"content\": f\"Interview topic: {topic}\"}]\n",
    "\n",
    "    # Generate the first question\n",
    "    prompt_question = f\"\"\"\n",
    "You are an expert interviewer. Using the following reference content:\n",
    "{content_list}\n",
    "\n",
    "Generate question #1 for the topic: {topic}.\n",
    "{'Ask a broad, general question.' if question_type.startswith('broad') else 'Ask a specific, detailed question.'}\n",
    "Return ONLY the question text.\n",
    "\"\"\"\n",
    "    first_question = call_gemini(prompt_question) or \"Tell me about your interest in this topic.\"\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"topic\": topic,\n",
    "        \"content\": content_list,\n",
    "        \"messages\": initial_messages,\n",
    "        \"step\": 0,\n",
    "        \"questions\": [],\n",
    "        \"answers\": [],\n",
    "        \"feedback\": [],\n",
    "        \"current_question\": first_question,\n",
    "        \"question_type\": question_type\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88d12a46-5c48-4a2f-8073-73db68b616ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_answer_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"Get candidate answer\"\"\"\n",
    "    current_q = state.get(\"current_question\")\n",
    "    if not current_q:\n",
    "        raise ValueError(\"No current_question found in state.\")\n",
    "\n",
    "    print(f\"\\nâ“ Generated Question {state['step'] + 1}: {current_q}\\n\")\n",
    "    answer = input(\"ðŸ’­ Your answer: \").strip()\n",
    "\n",
    "    new_messages = state['messages'] + [\n",
    "        {\"role\": \"interviewer\", \"content\": current_q},\n",
    "        {\"role\": \"candidate\", \"content\": answer}\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"current_answer\": answer,\n",
    "        \"messages\": new_messages,\n",
    "        \"questions\": state['questions'] + [current_q],\n",
    "        \"answers\": state['answers'] + [answer]\n",
    "    }\n",
    "\n",
    "def evaluate_question_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"Rate the quality of the last question and the candidate's answer.\"\"\"\n",
    "    # Compile transcript of previous Q&A and feedback\n",
    "    transcript = \"\"\n",
    "    for i in range(len(state['questions']) - 1):\n",
    "        q = state['questions'][i]\n",
    "        a = state['answers'][i]\n",
    "        f = state['feedback'][i] if i < len(state['feedback']) else {}\n",
    "        transcript += f\"Previous Q{i+1}: {q}\\nPrevious A{i+1}: {a}\\nPrevious Feedback: {f.get('question_feedback', {}).get('feedback', '')}\\n\\n\"\n",
    "\n",
    "    last_q = state['questions'][-1]\n",
    "    last_a = state['answers'][-1]\n",
    "    full_messages = json.dumps(state['messages'])\n",
    "    full_content = \"\\n\".join(state['content'])\n",
    "\n",
    "    # Question feedback prompt\n",
    "    question_prompt = f\"\"\"\n",
    "You are an expert interviewer. Evaluate the following question for its clarity, relevance, and ability to probe understanding, considering the ENTIRE interview history, accumulated context, all previous messages, questions, answers, and feedback.\n",
    "\n",
    "Full Interview History (Messages): {full_messages}\n",
    "Accumulated Context (Search Snippets): {full_content}\n",
    "Previous Q&A Transcript: {transcript}\n",
    "Current Question: {last_q}\n",
    "Current Candidate Answer: {last_a}\n",
    "\n",
    "Provide a rating (1-10) for question quality and 2-3 sentence feedback. Consider how well this question builds on prior answers, avoids repetition, incorporates context, and advances the topic.\n",
    "Return in JSON format:\n",
    "{{\n",
    "    \"rating\": 0,\n",
    "    \"feedback\": \"...\"\n",
    "}}\n",
    "\"\"\"\n",
    "    question_feedback_text = call_gemini(question_prompt)\n",
    "    question_feedback = safe_parse_json(question_feedback_text, {\"rating\": 0, \"feedback\": \"Failed to generate question feedback.\"})\n",
    "\n",
    "    # Answer feedback prompt\n",
    "    answer_prompt = f\"\"\"\n",
    "You are an expert interviewer. Evaluate the following candidate answer for its clarity, relevance, depth, and alignment with the question, considering the ENTIRE interview history and context.\n",
    "\n",
    "Full Interview History (Messages): {full_messages}\n",
    "Accumulated Context (Search Snippets): {full_content}\n",
    "Previous Q&A Transcript: {transcript}\n",
    "Current Question: {last_q}\n",
    "Current Candidate Answer: {last_a}\n",
    "\n",
    "Provide a rating (1-10) for answer quality and 2-3 sentence feedback. Highlight strengths and areas for improvement.\n",
    "Return in JSON format:\n",
    "{{\n",
    "    \"rating\": 0,\n",
    "    \"feedback\": \"...\"\n",
    "}}\n",
    "\"\"\"\n",
    "    answer_feedback_text = call_gemini(answer_prompt)\n",
    "    answer_feedback = safe_parse_json(answer_feedback_text, {\"rating\": 0, \"feedback\": \"Failed to generate answer feedback.\"})\n",
    "\n",
    "    feedback = {\n",
    "        \"question_feedback\": question_feedback,\n",
    "        \"answer_feedback\": answer_feedback\n",
    "    }\n",
    "    print(f\"ðŸ’¡ Question Feedback: {question_feedback['feedback']} (Rating: {question_feedback['rating']})\")\n",
    "    print(f\"ðŸ’¡ Answer Feedback: {answer_feedback['feedback']} (Rating: {answer_feedback['rating']})\")\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"feedback\": state['feedback'] + [feedback],\n",
    "        \"step\": state['step'] + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c96de2de-2b39-4a23-8b64-675d77c33382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_question_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"Generate next question based on type (broad/narrow, follow-up/non-follow-up).\"\"\"\n",
    "    updated_content = state['content']\n",
    "    question_type = state['question_type']\n",
    "    is_followup = \"followup\" in question_type\n",
    "    is_broad = question_type.startswith(\"broad\")\n",
    "\n",
    "    if state['step'] > 0:\n",
    "        last_q = state['questions'][-1]\n",
    "        last_a = state['answers'][-1]\n",
    "        tavily_results = search_tavily(f\"{state['topic']} interview context: Q: {last_q} A: {last_a}\")\n",
    "        updated_content += tavily_results\n",
    "\n",
    "    prompt_instruction = \"\"\n",
    "    if is_followup:\n",
    "        prompt_instruction = f\"Generate a {'broad, general' if is_broad else 'specific, detailed'} follow-up question that directly probes details from the previous answer: {state['answers'][-1] if state['answers'] else ''}.\"\n",
    "    else:\n",
    "        prompt_instruction = f\"Generate a {'broad, general' if is_broad else 'specific, detailed'} question that explores a new aspect of the topic, independent of the previous answer.\"\n",
    "\n",
    "    prompt_question = f\"\"\"\n",
    "You are an expert interviewer. Using the following reference content:\n",
    "{updated_content}\n",
    "\n",
    "{prompt_instruction}\n",
    "Topic: {state['topic']}\n",
    "Question number: {state['step'] + 1}\n",
    "Return ONLY the question text.\n",
    "\"\"\"\n",
    "    question = call_gemini(prompt_question) or f\"Tell me more about {state['topic']}.\"\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"current_question\": question,\n",
    "        \"content\": updated_content\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15028096-1604-4852-8ffa-9bb7068e47f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def final_evaluation_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"Generate final evaluation of all questions.\"\"\"\n",
    "    print(\"\\nðŸ“Š Generating final evaluation of all questions...\")\n",
    "    transcript = \"\"\n",
    "    for i, (q, a, f) in enumerate(zip(state['questions'], state['answers'], state['feedback']), 1):\n",
    "        transcript += f\"Q{i}: {q}\\nA{i}: {a}\\nQuestion Feedback: {f['question_feedback']['feedback']} (Rating: {f['question_feedback']['rating']})\\nAnswer Feedback: {f['answer_feedback']['feedback']} (Rating: {f['answer_feedback']['rating']})\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Based on this transcript, produce a JSON summary evaluation of the questions:\n",
    "{transcript}\n",
    "\n",
    "JSON format ONLY:\n",
    "{{\n",
    "    \"overall_quality\": 0-10,\n",
    "    \"strengths\": [\"...\"],\n",
    "    \"areas_for_improvement\": [\"...\"],\n",
    "    \"recommendation\": \"keep/revise/remove\",\n",
    "    \"final_feedback\": \"...\"\n",
    "}}\n",
    "\"\"\"\n",
    "    response_text = call_gemini(prompt)\n",
    "    evaluation = safe_parse_json(response_text, {\"overall_quality\": 0, \"recommendation\": \"revise\", \"final_feedback\": \"Failed to generate evaluation.\"})\n",
    "\n",
    "    return {**state, \"final_evaluation\": evaluation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6a73f26-7992-4aad-8011-7a6fd76cdf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_results_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"Display final report\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" INTERVIEW COMPLETE - FINAL REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n Topic: {state['topic']}\")\n",
    "    print(\"\\n Questions & Feedback:\")\n",
    "    for i, (q, a, f) in enumerate(zip(state['questions'], state['answers'], state['feedback']), 1):\n",
    "        print(f\"\\n{i}. {q}\")\n",
    "        print(f\"    Answer: {a}\")\n",
    "        print(f\"    Feedback: {f}\")\n",
    "\n",
    "    print(\"\\nðŸ“Š Final Evaluation:\")\n",
    "    eval_data = state['final_evaluation']\n",
    "    if \"error\" in eval_data:\n",
    "        print(\" Could not parse evaluation:\", eval_data[\"error\"])\n",
    "    else:\n",
    "        for k, v in eval_data.items():\n",
    "            print(f\"   {k}: {v}\")\n",
    "\n",
    "    with open(\"interview_results.json\", \"w\") as f:\n",
    "        json.dump(state, f, indent=2)\n",
    "    print(\"\\n Results saved to 'interview_results.json'\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cdf26-cb05-4c4b-af1a-f133f46052f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90689ef5-486f-4e5c-a1b3-9c81efc39293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Graph ----\n",
    "def should_continue(state: InterviewState) -> str:\n",
    "    return \"generate_question\" if state['step'] < state['max_questions'] else \"final_evaluation\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01a3474e-369b-4eb6-9291-a89396601c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "builder = StateGraph(InterviewState)\n",
    "builder.add_node(\"setup\", setup_node)\n",
    "builder.add_node(\"get_answer\", get_answer_node)\n",
    "builder.add_node(\"evaluate_question\", evaluate_question_node)\n",
    "builder.add_node(\"generate_question\", generate_question_node)\n",
    "builder.add_node(\"final_evaluation\", final_evaluation_node)\n",
    "builder.add_node(\"display_results\", display_results_node)\n",
    "\n",
    "builder.set_entry_point(\"setup\")\n",
    "builder.add_edge(\"setup\", \"get_answer\")\n",
    "builder.add_edge(\"get_answer\", \"evaluate_question\")\n",
    "builder.add_conditional_edges(\n",
    "    \"evaluate_question\",\n",
    "    should_continue,\n",
    "    {\"generate_question\": \"generate_question\", \"final_evaluation\": \"final_evaluation\"}\n",
    ")\n",
    "builder.add_edge(\"generate_question\", \"get_answer\")\n",
    "builder.add_edge(\"final_evaluation\", \"display_results\")\n",
    "builder.add_edge(\"display_results\", END)\n",
    "\n",
    "interview_graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0f182-c556-47b2-b2a9-58772133c151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Welcome to the AI Interviewer (Question Evaluation Mode)!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Run ----\n",
    "if __name__ == \"__main__\":\n",
    "    initial_state = {\n",
    "        \"topic\": \"\",\n",
    "        \"content\": [],\n",
    "        \"questions\": [],\n",
    "        \"answers\": [],\n",
    "        \"feedback\": [],\n",
    "        \"current_question\": None,\n",
    "        \"current_answer\": None,\n",
    "        \"step\": 0,\n",
    "        \"max_questions\": 3,\n",
    "        \"final_evaluation\": None,\n",
    "        \"messages\": [],\n",
    "        \"question_type\": \"broad_followup\"\n",
    "    }\n",
    "    final_state = interview_graph.invoke(initial_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
